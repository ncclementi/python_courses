{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polish assignment, write it nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://python-data.dr-chuck.net/known_by_Fikret.html\n"
     ]
    }
   ],
   "source": [
    "url = raw_input('Enter - ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montgomery 1\n",
      "Mhairade 2\n",
      "Butchi 3\n",
      "Anayah 4\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "while count <= 4:          \n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a') \n",
    "    name = tags[2].contents[0]\n",
    "    url = tags[2].get('href', None)\n",
    "    \n",
    "    print name, count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://python-data.dr-chuck.net/known_by_Ellise.html\n"
     ]
    }
   ],
   "source": [
    "url = raw_input('Enter - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daud 1\n",
      "Gareth 2\n",
      "Henri 3\n",
      "Karli 4\n",
      "Eeman 5\n",
      "Korrin 6\n",
      "Declan 7\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "while count <= 7:          \n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a') \n",
    "    name = tags[17].contents[0]\n",
    "    url = tags[17].get('href', None)\n",
    "    \n",
    "    print name, count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncclementi/anaconda/envs/py2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/ncclementi/anaconda/envs/py2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "#tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents: Aniqa\n",
      "Contents: Ogheneruno\n",
      "Contents: Montgomery\n",
      "Contents: Owain\n",
      "Contents: Haniyah\n",
      "Contents: Anona\n",
      "Contents: Edyn\n",
      "Contents: Dallace\n",
      "Contents: Zoe\n",
      "Contents: Kiarash\n",
      "Contents: Tracy\n",
      "Contents: Carmyle\n",
      "Contents: Zahraa\n",
      "Contents: Alanys\n",
      "Contents: Airidas\n",
      "Contents: Melisa\n",
      "Contents: Vivian\n",
      "Contents: Margaret\n",
      "Contents: Hajra\n",
      "Contents: Ajooni\n",
      "Contents: Alexanne\n",
      "Contents: Sudais\n",
      "Contents: Seb\n",
      "Contents: Christin\n",
      "Contents: Jaimie\n",
      "Contents: Jennah\n",
      "Contents: Landon\n",
      "Contents: Mea\n",
      "Contents: Cacie\n",
      "Contents: Colton\n",
      "Contents: Mitchel\n",
      "Contents: Chintu\n",
      "Contents: Hyden\n",
      "Contents: Chrystal\n",
      "Contents: Lincon\n",
      "Contents: Jaden\n",
      "Contents: Roma\n",
      "Contents: Manolo\n",
      "Contents: Clio\n",
      "Contents: Teos\n",
      "Contents: Rihonn\n",
      "Contents: Griffin\n",
      "Contents: Conley\n",
      "Contents: Xiao\n",
      "Contents: Dhyia\n",
      "Contents: Manahil\n",
      "Contents: Diona\n",
      "Contents: Dharam\n",
      "Contents: Danielle\n",
      "Contents: Rori\n",
      "Contents: Lang\n",
      "Contents: Sabila\n",
      "Contents: Zoha\n",
      "Contents: Jemma\n",
      "Contents: Silvana\n",
      "Contents: Asal\n",
      "Contents: Dillon\n",
      "Contents: CJ\n",
      "Contents: Joanna\n",
      "Contents: Atal\n",
      "Contents: Callun\n",
      "Contents: Anubhav\n",
      "Contents: Coray\n",
      "Contents: Graeme\n",
      "Contents: Chrissie\n",
      "Contents: Ayub\n",
      "Contents: Heather\n",
      "Contents: Katie\n",
      "Contents: Inaara\n",
      "Contents: Siddhant\n",
      "Contents: Salymat\n",
      "Contents: Shahd\n",
      "Contents: Anaya\n",
      "Contents: Kevaugh\n",
      "Contents: Thumbiko\n",
      "Contents: Xida\n",
      "Contents: Alaska\n",
      "Contents: Shonagh\n",
      "Contents: Kaiya\n",
      "Contents: Khadija\n",
      "Contents: Kieron\n",
      "Contents: Filip\n",
      "Contents: Dorothy\n",
      "Contents: Kallan\n",
      "Contents: Mena\n",
      "Contents: Abbie\n",
      "Contents: Amyleigh\n",
      "Contents: Annalise\n",
      "Contents: Carrich\n",
      "Contents: Rachel\n",
      "Contents: Etinosa\n",
      "Contents: Amie\n",
      "Contents: Lisa\n",
      "Contents: Liv\n",
      "Contents: Baylie\n",
      "Contents: Jubin\n",
      "Contents: Kacie\n",
      "Contents: Falyn\n",
      "Contents: Conli\n",
      "Contents: Cohen\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all of the anchor tags\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "#print 'TAG:',tags[2]\n",
    "#print 'URL:',tags[2].get('href', None)\n",
    "    print 'Contents:',tag.contents[0]\n",
    "#print 'Attrs:',tags[2].attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
